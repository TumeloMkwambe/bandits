# Multi-Armed Bandit: Action Selection Methods

![Experiment 1 Results](experiment_1.png)

This project compares the performance of three action selection strategies in a **k-armed bandit problem**:

- **Greedy**
- **ε-Greedy**
- **Upper Confidence Bound (UCB)**

The goal is to evaluate how each algorithm performs over time in terms of **average reward**.

## Experiment 1 Setup

- **Number of actions**: 10
- **Time steps per run**: 1000
- **Total runs**: 10,000
- **Initial estimated value (Q₁)**: 5 for all actions
- **ε (epsilon for exploration)**: 0.1
- **c (UCB exploration parameter)**: 2

## Experiment 2 Setup
    ***
    ***
    ***
